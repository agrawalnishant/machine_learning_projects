{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get SVHN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import cPickle as pickle\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Load SVHN Data files, including mat data, for train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posix.stat_result(st_mode=33204, st_ino=665840, st_dev=51713L, st_nlink=1, st_uid=1000, st_gid=1000, st_size=404141560, st_atime=1472397275, st_mtime=1472109618, st_ctime=1472109618)\n",
      "Found and verified train.tar.gz\n",
      "posix.stat_result(st_mode=33204, st_ino=665841, st_dev=51713L, st_nlink=1, st_uid=1000, st_gid=1000, st_size=276555967, st_atime=1473490141, st_mtime=1472109703, st_ctime=1472109703)\n",
      "Found and verified test.tar.gz\n",
      "posix.stat_result(st_mode=33204, st_ino=718195, st_dev=51713L, st_nlink=1, st_uid=1000, st_gid=1000, st_size=1955489752, st_atime=1473490198, st_mtime=1473490427, st_ctime=1473490427)\n",
      "Found and verified extra.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "last_percent_reported = None\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 1% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(filename)\n",
    "  print(statinfo)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "train_filename = maybe_download('train.tar.gz',404141560)\n",
    "test_filename = maybe_download('test.tar.gz',276555967)\n",
    "extra_filename = maybe_download('extra.tar.gz', 1955489752)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already present - Skipping extraction of train.tar.gz.\n",
      "train\n",
      "test already present - Skipping extraction of test.tar.gz.\n",
      "test\n",
      "Extracting data for extra. This may take a while. Please wait.\n",
      "extra\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = root\n",
    "  \n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n",
    "extra_folders = maybe_extract(extra_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Format .mat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time \n",
    "\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "class ImageData:\n",
    "    def __init__(self, data_folder, imageDataMatFilePath):\n",
    "        self.data_folder=data_folder\n",
    "        self.file = os.path.join(data_folder, 'digitStruct.mat')\n",
    "        self.file = h5py.File(self.file, 'r')\n",
    "        self.names = self.file['digitStruct/name']\n",
    "        self.boxes = self.file['digitStruct/bbox']\n",
    "    \n",
    "    def get_labels_and_dataset(self, is_train):\n",
    "        image_vertices_count=4\n",
    "        \n",
    "        ''' 5 places + 1 for length'''\n",
    "        digit_count=6\n",
    "        desired_image_size=32\n",
    "\n",
    "        #Images to be processed\n",
    "        image_count = len(self.names)\n",
    "\n",
    "        #Array to hold labels of digits in an image\n",
    "        #Add one for length\n",
    "        print(\"is_train:\",is_train)\n",
    "        if (is_train == True) :\n",
    "            multiplier = 2\n",
    "        else:\n",
    "            multiplier = 1\n",
    "        image_labels_array=np.zeros(multiplier * image_count * (digit_count)).reshape(multiplier * image_count,digit_count)\n",
    "        dataset_basic = np.ndarray(shape=(multiplier * image_count, desired_image_size, desired_image_size),dtype=np.float32)\n",
    "\n",
    "        print(\"image_labels: \",image_labels_array.shape)\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        ##Iterate over all images and get dims of images's bbox ( and not digit bbox), and digit labels.\n",
    "        for count in range(image_count):\n",
    "            left=0.\n",
    "            top=0.\n",
    "            bottom=0.\n",
    "            right=0.\n",
    "            img_label=self.file[self.boxes[count].item()][\"label\"]\n",
    "            img_left=self.file[self.boxes[count].item()][\"left\"]\n",
    "            img_top=self.file[self.boxes[count].item()][\"top\"]\n",
    "            img_height=self.file[self.boxes[count].item()][\"height\"]\n",
    "            img_width=self.file[self.boxes[count].item()][\"width\"]\n",
    "            num_digits=len(img_label)\n",
    "            #print(\"num_digits:\", num_digits)\n",
    "\n",
    "            \n",
    "            \n",
    "            image_labels_array[count,:]=float(0)\n",
    "            image_labels_array[count,0]=num_digits\n",
    "            ##Get bbox and digits for each image\n",
    "            for digit_counter in np.arange(num_digits):\n",
    "                if(num_digits == 1):\n",
    "                    tmp_label=img_label.value[0]\n",
    "                    image_labels_array[count,digit_count - num_digits + digit_counter]=tmp_label\n",
    "                    tmp_left=img_left.value[0]\n",
    "                    tmp_top=img_top.value[0]\n",
    "                    tmp_height=img_height.value[0]\n",
    "                    tmp_width=img_width.value[0]\n",
    "                else:\n",
    "                    tmp_label=self.file[img_label.value[digit_counter].item()].value[0]\n",
    "                    image_labels_array[count,digit_count - num_digits + digit_counter]=tmp_label\n",
    "                    tmp_left=self.file[img_left.value[digit_counter].item()].value[0]\n",
    "                    tmp_top=self.file[img_top.value[digit_counter].item()].value[0]\n",
    "                    tmp_height=self.file[img_height.value[digit_counter].item()].value[0]\n",
    "                    tmp_width=self.file[img_width.value[digit_counter].item()].value[0]\n",
    "                #print(\"tmp_label:\", tmp_label)\n",
    "\n",
    "                tmp_bottom=tmp_top+tmp_height\n",
    "                tmp_right=tmp_left+tmp_width\n",
    "\n",
    "                #Find the largest margins by taking min of top and left, and max of right and bottom\n",
    "                #Also reduce top and left margins, and increase right and bottom margings to avoid cutting digits\n",
    "                # cases handling x==0 are for initialization\n",
    "                if(left==0 or tmp_left < left):\n",
    "                    left=tmp_left*0.95;\n",
    "                if(top==0 or tmp_top < top):\n",
    "                    top=tmp_top*0.95\n",
    "                if(bottom ==0 or tmp_bottom > bottom):\n",
    "                    bottom=tmp_bottom*1.05\n",
    "                if(right==0 or tmp_right > right):\n",
    "                    right=tmp_right*1.05\n",
    "            fullname = os.path.join(self.data_folder, str(count+1)+\".png\")\n",
    "            im = Image.open(fullname)\n",
    "            #plt.imshow(im)\n",
    "            #im.show()\n",
    "            im_orig = im.crop((left, top, right, bottom)).resize([desired_image_size,desired_image_size]).convert('L')\n",
    "            dataset_basic[count]=im_orig\n",
    "            #plt.imshow(im_orig, cmap='Greys_r')\n",
    "                       \n",
    "        print(\"image_labels_array: \",image_labels_array.shape, \"\\n shape: \",image_labels_array[0:2,:],\\\n",
    "              \",\\n\",image_labels_array[image_count:image_count+2,:])\n",
    "      \n",
    "        print(\"Time taken to find image bounding box:\\n- %4.4f seconds ---\" % (time.time() - start_time))\n",
    "        print(\"\\nsample:\\n\",dataset_basic[0:1,:])\n",
    "        dataset_basic = (dataset_basic.astype(float) - pixel_depth / 2) / pixel_depth\n",
    "        print(\"\\nsample:\\n\",dataset_basic[0:1,:])\n",
    "        print('Mean:', np.mean(dataset_basic))\n",
    "        print('Standard deviation:', np.std(dataset_basic))\n",
    "        \n",
    "        \n",
    "        return image_labels_array, dataset_basic \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_train: False\n",
      "image_labels:  (33402, 6)\n",
      "image_labels_array:  (33402, 6) \n",
      " shape:  [[ 2.  0.  0.  0.  1.  9.]\n",
      " [ 2.  0.  0.  0.  2.  3.]] ,\n",
      " []\n",
      "Time taken to find image bounding box:\n",
      "- 214.6577 seconds ---\n",
      "\n",
      "sample:\n",
      " [[[  73.   75.   74. ...,   78.   83.   79.]\n",
      "  [  78.   76.   74. ...,   78.   81.   79.]\n",
      "  [  79.   81.   77. ...,   81.   84.   83.]\n",
      "  ..., \n",
      "  [ 101.  101.  103. ...,  103.  100.  102.]\n",
      "  [ 101.  102.  102. ...,  102.  105.  102.]\n",
      "  [  80.  108.  104. ...,  104.  101.   99.]]]\n",
      "\n",
      "sample:\n",
      " [[[-0.21372549 -0.20588235 -0.20980392 ..., -0.19411765 -0.1745098\n",
      "   -0.19019608]\n",
      "  [-0.19411765 -0.20196078 -0.20980392 ..., -0.19411765 -0.18235294\n",
      "   -0.19019608]\n",
      "  [-0.19019608 -0.18235294 -0.19803922 ..., -0.18235294 -0.17058824\n",
      "   -0.1745098 ]\n",
      "  ..., \n",
      "  [-0.10392157 -0.10392157 -0.09607843 ..., -0.09607843 -0.10784314 -0.1       ]\n",
      "  [-0.10392157 -0.1        -0.1        ..., -0.1        -0.08823529 -0.1       ]\n",
      "  [-0.18627451 -0.07647059 -0.09215686 ..., -0.09215686 -0.10392157\n",
      "   -0.11176471]]]\n",
      "Mean: -0.0566795406976\n",
      "Standard deviation: 0.19967425856\n"
     ]
    }
   ],
   "source": [
    "train_image_data = ImageData(train_folders, 'digitStruct.mat')\n",
    "train_image_labels, train_dataset_basic = train_image_data.get_labels_and_dataset(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Pickle Train Sets\n",
    "train_pickle_file = 'SVHN_basic_train_labels.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(train_pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_image_labels': train_image_labels\n",
    "    }\n",
    "  pickle.dump(save, f, 1)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save train label data to', train_pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed train pickle size: 273629347\n"
     ]
    }
   ],
   "source": [
    "##Pickle Train Sets\n",
    "train_pickle_file = 'SVHN_basic_train_data_basic.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(train_pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset_basic\n",
    "    }\n",
    "  pickle.dump(save, f, 1)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', train_pickle_file, ':', e)\n",
    "  raise\n",
    "\n",
    "statinfo = os.stat(train_pickle_file)\n",
    "print('Compressed train pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_train: False\n",
      "image_labels:  (13068, 6)\n",
      "image_labels_array:  (13068, 6) \n",
      " shape:  [[  1.   0.   0.   0.   0.   5.]\n",
      " [  3.   0.   0.   2.   1.  10.]] ,\n",
      " []\n",
      "Time taken to find image bounding box:\n",
      "- 94.7911 seconds ---\n",
      "\n",
      "sample:\n",
      " [[[ 79.  79.  79. ...,  79.  78.  78.]\n",
      "  [ 79.  79.  80. ...,  79.  77.  78.]\n",
      "  [ 79.  79.  80. ...,  79.  79.  78.]\n",
      "  ..., \n",
      "  [ 90.  90.  83. ...,  90.  90.  89.]\n",
      "  [ 87.  87.  80. ...,  91.  90.  90.]\n",
      "  [ 85.  85.  79. ...,  90.  89.  88.]]]\n",
      "\n",
      "sample:\n",
      " [[[-0.19019608 -0.19019608 -0.19019608 ..., -0.19019608 -0.19411765\n",
      "   -0.19411765]\n",
      "  [-0.19019608 -0.19019608 -0.18627451 ..., -0.19019608 -0.19803922\n",
      "   -0.19411765]\n",
      "  [-0.19019608 -0.19019608 -0.18627451 ..., -0.19019608 -0.19019608\n",
      "   -0.19411765]\n",
      "  ..., \n",
      "  [-0.14705882 -0.14705882 -0.1745098  ..., -0.14705882 -0.14705882\n",
      "   -0.15098039]\n",
      "  [-0.15882353 -0.15882353 -0.18627451 ..., -0.14313725 -0.14705882\n",
      "   -0.14705882]\n",
      "  [-0.16666667 -0.16666667 -0.19019608 ..., -0.14705882 -0.15098039\n",
      "   -0.15490196]]]\n",
      "Mean: -0.0510183666569\n",
      "Standard deviation: 0.225976380396\n"
     ]
    }
   ],
   "source": [
    "test_image_data = ImageData(test_folders, 'digitStruct.mat')\n",
    "test_imagee_labels, test_dataset_basic = test_image_data.get_labels_and_dataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Pickle Test Sets\n",
    "test_pickle_file = 'SVHN_basic_test_labels.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(test_pickle_file, 'wb')\n",
    "  save = {\n",
    "    'test_image_labels': test_imagee_labels\n",
    "    }\n",
    "  pickle.dump(save, f, 1)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save test label data to', test_pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed test data pickle size: 107053218\n"
     ]
    }
   ],
   "source": [
    "##Pickle Test Sets\n",
    "test_pickle_file = 'SVHN_basic_test_data_basic.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(test_pickle_file, 'wb')\n",
    "  save = {\n",
    "    'test_dataset': test_dataset_basic\n",
    "    }\n",
    "  pickle.dump(save, f, 1)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', test_pickle_file, ':', e)\n",
    "  raise\n",
    "\n",
    "statinfo = os.stat(test_pickle_file)\n",
    "print('Compressed test data pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_train: False\n",
      "image_labels:  (202353, 6)\n",
      "image_labels_array:  (202353, 6) \n",
      " shape:  [[ 3.  0.  0.  4.  7.  8.]\n",
      " [ 2.  0.  0.  0.  7.  1.]] ,\n",
      " []\n",
      "Time taken to find image bounding box:\n",
      "- 1468.9432 seconds ---\n",
      "\n",
      "sample:\n",
      " [[[ 42.  44.  44. ...,  46.  44.  45.]\n",
      "  [ 42.  43.  43. ...,  47.  45.  44.]\n",
      "  [ 42.  42.  43. ...,  46.  43.  44.]\n",
      "  ..., \n",
      "  [ 45.  47.  47. ...,  47.  47.  48.]\n",
      "  [ 46.  46.  45. ...,  46.  49.  49.]\n",
      "  [ 46.  44.  44. ...,  49.  47.  48.]]]\n",
      "\n",
      "sample:\n",
      " [[[-0.33529412 -0.32745098 -0.32745098 ..., -0.31960784 -0.32745098\n",
      "   -0.32352941]\n",
      "  [-0.33529412 -0.33137255 -0.33137255 ..., -0.31568627 -0.32352941\n",
      "   -0.32745098]\n",
      "  [-0.33529412 -0.33529412 -0.33137255 ..., -0.31960784 -0.33137255\n",
      "   -0.32745098]\n",
      "  ..., \n",
      "  [-0.32352941 -0.31568627 -0.31568627 ..., -0.31568627 -0.31568627\n",
      "   -0.31176471]\n",
      "  [-0.31960784 -0.31960784 -0.32352941 ..., -0.31960784 -0.30784314\n",
      "   -0.30784314]\n",
      "  [-0.31960784 -0.32745098 -0.32745098 ..., -0.30784314 -0.31568627\n",
      "   -0.31176471]]]\n",
      "Mean: -0.0725734750725\n",
      "Standard deviation: 0.196826622763\n"
     ]
    }
   ],
   "source": [
    "extra_image_data = ImageData(extra_folders, 'digitStruct.mat')\n",
    "extra_imagee_labels, extra_dataset_basic = extra_image_data.get_labels_and_dataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Pickle extra Sets\n",
    "extra_pickle_file = 'SVHN_basic_extra_labels.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(extra_pickle_file, 'wb')\n",
    "  save = {\n",
    "    'extra_image_labels': extra_imagee_labels\n",
    "    }\n",
    "  pickle.dump(save, f, 1)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save extra label data to', extra_pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed extra data pickle size: 1657675941\n"
     ]
    }
   ],
   "source": [
    "##Pickle Test Sets\n",
    "extra_pickle_file = 'SVHN_basic_extra_data_basic.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(extra_pickle_file, 'wb')\n",
    "  save = {\n",
    "    'extra_dataset': extra_dataset_basic\n",
    "    }\n",
    "  pickle.dump(save, f, 1)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', extra_pickle_file, ':', e)\n",
    "  raise\n",
    "\n",
    "statinfo = os.stat(extra_pickle_file)\n",
    "print('Compressed extra data pickle size:', statinfo.st_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
